{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> GUIDE </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onehot Encoding and dropping obvious first feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding and dropping first feature\n",
    "\n",
    "def enc(feature):\n",
    "    dummy = pd.get_dummies(df_train[[feature]],drop_first=True)\n",
    "    return dummy\n",
    "\n",
    "airline_dummy = enc('Airline')\n",
    "source_dummy = enc('Source')\n",
    "destination_dummy = enc('Destination')\n",
    "total_stops_dummy = enc('Total_Stops')\n",
    "\n",
    "# Concat encoded columms to original dataframe\n",
    "\n",
    "df_train_new = pd.concat([df_train,airline_dummy,source_dummy,destination_dummy,total_stops_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatPlot - Categorical vs continuous plotting to check outliers and value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y='Price',x='Airline',data=df_train.sort_values('Price',ascending=False),kind='boxen',height=6,aspect=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap - plotting heatmap for correlation matrix using matplotlib and seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,12))\n",
    "sns.heatmap(df_train1.corr(), annot=True, cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Collinearity - Variation Inflation factor for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity VIF\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def vif(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['variables'] = X.columns\n",
    "    vif['vif'] = [variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics from Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('MSE',metrics.mean_squared_error(y_test,y_pred))\n",
    "print('R2',metrics.r2_score(y_test,y_pred))\n",
    "print('Adjusted R2', 1 - ((1 - metrics.r2_score(y_test,y_pred)) * (len(X_test) - 1) / (len(X_test) - X_test.shape[1] - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation(CV) - K-fold RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "# Number of features to considers at every split\n",
    "max_features = ['auto','sqrt']\n",
    "# Maximum number of levels in a tree\n",
    "max_depth = [int(x) for x in np.linspace(start=10, stop=110, num=11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2,5,10]\n",
    "# Minimum number of samples required at leaf node\n",
    "min_samples_leaf = [1,2,4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create Random Grid\n",
    "random_grid = {'n_estimators':n_estimators,\n",
    "              'max_features':max_features,\n",
    "              'max_depth':max_depth,\n",
    "              'min_samples_split':min_samples_split,\n",
    "              'min_samples_leaf':min_samples_leaf,\n",
    "              'bootstrap':bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "rf = RandomForestRegressor\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3,\n",
    "                              verbose = 2, random_state = 42, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train,y_train)\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_random.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model as Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model as pickle file\n",
    "import pickle\n",
    "\n",
    "file = open('flight.pkl', 'wb')\n",
    "\n",
    "pickle.dump(rf_random, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model from pickle file\n",
    "import pickle \n",
    "\n",
    "model = open('flight.pkl', 'rb')\n",
    "\n",
    "rf_model = load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient based algorithms: \n",
    "    Algorithms like Linear Regression, Logistic Regression and Neural networks,etc. that use gradient descent as optimization technique require data to be scaled. \"Have features on same scale can help the gradient descent to converge more quickly towards global minima\".\n",
    "\n",
    "Distance based algorithms:\n",
    "    Algorithms like SVM, KNN, K-means,etc. are more affected by the range of features. This is because behind the scenes, they are calculating distance between data points to determine their similarity.\n",
    "    \n",
    "Tree base algorithms are fairly insensitive to the scale of features. Since there is no effect of other feature on split of a node in tree based algorithms, they are not effected by scale.\n",
    "    \n",
    "Two types of Feature scaling techniques:\n",
    "    1. Normalization\n",
    "    2. Standardization\n",
    "\n",
    "Normalization: (ranges between 0 and 1) Normalization is good when your data is not following a gaussian distribution.\n",
    "    Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging from 0 and 1. It is also known as Min Max Scaling.\n",
    "    \n",
    "    X' = (X - Xmin) / (Xmax-Xmin)\n",
    "    where, Xmin = minimum value of the feature\n",
    "           Xmax = maximum value of the feature\n",
    "           X = Present value in the feature\n",
    "           X' = New scaled value in the feature\n",
    "           \n",
    "Standardization: standardization is good when data is following gaussian distribution.\n",
    "    It is another scaling technique where the values are centered around mean with a unit standard deviation. This means mean of the attribute becomes 0 and resultant standard deviation becomes 1.\n",
    "    \n",
    "    X' = (X - μ)/σ\n",
    "    where, X' = New scaled value in the feature\n",
    "           X = Present value in the feature\n",
    "           μ = mean\n",
    "           σ = standard deviation\n",
    "           \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally define outliers as samples that are exceptionally far from the mainstream of the data.\n",
    "\n",
    "Three ways to detect outliers:\n",
    "    1. Standard Deviation method\n",
    "    2. Inter Quartile range method\n",
    "    3. Automatic outlier detection\n",
    "    \n",
    "Standard Deviation method:\n",
    "    If we know that the distribution of values in a sample is gaussian or gaussian-like distribution, we can use the standard deviation of the sample as a cut off for identifying outliers.\n",
    "    1. 1 standard deviation from the mean : 68%\n",
    "    2. 2 standard deviation from the mean : 95%\n",
    "    3. 3 standard deviation from the mean : 99.7%\n",
    "A value that falls outside of the 3 standard deviations is part of distribution, but an unlikely or rare event.\n",
    "Three standard deviations from mean as cut off is a standard practice for gaussian or gaussian like distributions. For small sample we use 2 standard deviations, for large samples we can use upto 4 standard deviations.\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    mean, std = np.mean(data), np.std(data)\n",
    "    cut_off = std * 3\n",
    "    lower, upper = mean - cut_off, mean + cut_off\n",
    "    \n",
    "    outliers = [x for x in data if x < lower or x > lower]\n",
    "    outliers_removed = [x for x in data if x > lower and x < upper]\n",
    "\n",
    "\n",
    "Inter Quarantile Range(IQR):\n",
    "    Not all data is normally distributed or not normal enough to treat it as being drawn from gaussian distribution.\n",
    "    A good statistic for summarizing non-gaussian distribution data is the interquartile range. Inter quartile range is the difference between 75 and 25 percentile of the data and defines the box in boxplot or whisker plot. Percentiles can be calculated by sorting the observations and selecting values at specific indices. The 50th percentile is the middle value or the average of 5000 and 5001 index value in a 10000 rows data.\n",
    "    IQR defines middle 50% of the data, or the data body.\n",
    "    \n",
    " \n",
    "    import numpy as np\n",
    "    q25, q75 = np.percentile(data,25), np.percentile(data,75)\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    cut_off = iqr * 1.5\n",
    "    lower = q25 - cut_off\n",
    "    upper = q75 + cut_off\n",
    "    \n",
    "    # use limits\n",
    "    outliers = [x for x in data if x < lower or x > lower]\n",
    "    outliers_removed = [x for x in data if x > lower and x < upper]\n",
    "\n",
    "    \n",
    "Automatic Outlier Detection:\n",
    "    In machine learning, an approach to tackling the problem of outlier detection is one-class classification. One class classification or OOC for short, involves fitting a model on the normal data and predicting if new data is normal or outlier/anamoly.\n",
    "    The local outlier factor or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a score of how isolated it is based on the size of the neighborhood.\n",
    "\n",
    "    lof = LocalOutlierFactor()\n",
    "    yhat = lof.fit_predict(X_train)\n",
    "    \n",
    "    # select all rows that are not outliers \n",
    "    mask = yhat != -1\n",
    "    X_train,y_train = X_train[mask, :], y_train[mask]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Gaussian Distribution or Normal distribution test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of scenarios:\n",
    "1. If input feature is numerical, use distplot or box plot or qqplot or Kolmogorov Smirnov test or Shapiro-Wilk test\n",
    "2. If input feature is categorical, go directly to feature selection like corr matrix with heatmap or anova test is output feature is numerical, if output feature is categorical, go with Chi-square test. \n",
    "\n",
    "    sns.distplot(df['feature'])\n",
    "\n",
    "or\n",
    "\n",
    "shapiro-wilk test: if p-value is greater than 0.05, we assume normal distribution, if it is lower, we do not assume gaussian distribution.\n",
    "\n",
    "    from statsmodels.stats import shapiro\n",
    "    shapiro(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection of Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. input numerical and output numerical -- correlation matrix(pearson correlation coefficient), VIF\n",
    "2. input categorical and output numerical -- ANOVA test\n",
    "3. input numerical and output categorical -- ANOVA test\n",
    "4. input categorical and output cactegorical -- Chi2 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'normalize':[True,False]}\n",
    "lr_cv_model = GridSearchCV(model,parameters,refit=True,cv=5)\n",
    "best_model = lr_cv_model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM and Tree based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No normality assumptions are needed for tree based models and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of classifications:</br>\n",
    "    1. Binary Classification - Logistic Regression, K-Nearest Neighbors, Decision trees, Support vector machine, Naive Bayes</br>\n",
    "    2. Multiclass Classification - K-Nearest Neighbors, Decision trees, Naive Bayes, Random Forest, Gradient Boosting</br>\n",
    "    3. Imbalanced Classification - Usually observed in binary classification datasets where normal or abnormal samples \n",
    "    are not in same ratio.</br>\n",
    "        When we encounter unbalenced binary classification, use Random undersampling or SMOTE oversampling to balance the \n",
    "        dataset and then use cost sensitive logistic regression or cost sensitive Decision trees or cost sensitive SVM and use performance metrics like precision, recall and f-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Algorithim for classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Algorithm for classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression for binary classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors(KNN) for classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine(SVM) for classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes for classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting algorithm for classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
